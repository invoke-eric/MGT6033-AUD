# Answer Key

## Section 1: Latent Dirichlet Allocation (LDA) Concepts

1. **Correct Answer:** B  
**Explanation:** "Latent" in LDA refers to the hidden or implicit topics in the corpus.  
**Verbatim Quote:** "Latent implies that we're looking for something hidden or implicit. Those hidden or implicit references are the topics in the corpus." (5.2b-LDA-NMF.txt)

2. **Correct Answers:** A, B, D  
**Explanation:** LDA assumes each word occurs with some probability within each topic, each topic occurs with some probability in each document, and both are governed by Dirichlet priors.  
**Verbatim Quote:** "First, each word occurs with some probability within each topic. Second, each topic occurs with some probability throughout each document. ... Dirichlet prior is an assumption made about the distribution of parameters before observing any data." (5.2b-LDA-NMF.txt)

3. **Correct Answer:** B  
**Explanation:** The Dirichlet distribution acts as a prior for the topic and word distributions in LDA.  
**Verbatim Quote:** "Dirichlet is a multivariate probability distribution that's often used in Bayesian statistics. In the context of LDA, this distribution is used as a prior distribution for the topic and word distributions." (5.2b-LDA-NMF.txt)

4. **Correct Answers:** A, B, C  
**Explanation:** Alpha is the topic prior (topic distribution in documents), Beta is the word prior (word distribution in topics), and higher Alpha leads to more uniform topic distributions; lower Beta leads to sparser, not more uniform, word distributions.  
**Verbatim Quote:** "Alpha is called the topic prior. ... A higher value for Alpha leads to documents that are more uniform in representation of topics ... The word prior or Beta is the Dirichlet prior for the distribution of words within a topic." (5.2b-LDA-NMF.txt)

5. **Correct Answer:** B  
**Explanation:** LDA requires integer word counts, not TF-IDF values.  
**Verbatim Quote:** "Note that the Dirichlet distribution requires integer values, so our document term matrix must be simple word counts. We cannot use TF-IDF with LDA." (5.2b-LDA-NMF.txt)

## Section 2: Non-negative Matrix Factorization (NMF) Concepts

6. **Correct Answer:** A  
**Explanation:** "Non-negative" means the matrix contains only positive or zero values.  
**Verbatim Quote:** "Non-negative: Contains only positive or 0 values" (5.2b.pdf)

7. **Correct Answers:** A, B  
**Explanation:** NMF produces a document-topic matrix (W) and a topic-word matrix (H).  
**Verbatim Quote:** "The two lower-dimension matrices are topic-word matrix (H), and document-topic matrix (W)" (5.2b.pdf)

8. **Correct Answer:** A  
**Explanation:** NMF decomposes the document-term matrix into two smaller, interpretable matrices.  
**Verbatim Quote:** "The main intuition is that a larger matrix can be represented as the product of two smaller matrices." (5.2b-LDA-NMF.txt)

9. **Correct Answers:** A, B, C  
**Explanation:** Both LDA and NMF produce topic-word and document-topic matrices, and both require the number of topics to be specified. They do not guarantee identical results.  
**Verbatim Quote:** "Both LDA and NMF require us to choose the number of topics." (5.2b.pdf)

10. **Correct Answer:** A  
**Explanation:** LDA is Bayesian and generative, NMF is a mathematical factorization method.  
**Verbatim Quote:** "LDA is a model derived using Bayesian methods that assumes document generation follows a specific process ... NMF is a purely mathematical approach to topic modeling that leverages the non-negative nature of document-term matrices." (5.2b.pdf)
