{
  "title": "Quiz Questions",
  "sections": [
    {
      "title": "Section 1: Data Preparation and Feature Engineering",
      "questions": [
        {
          "number": 1,
          "difficulty": "Easy",
          "type": "single_select",
          "question": "Which vectorizer is used to generate the document-term matrix (DTM) for clustering in Demo 5B?",
          "options": {
            "A": "CountVectorizer",
            "B": "TfidfVectorizer",
            "C": "HashingVectorizer",
            "D": "DictVectorizer"
          }
        },
        {
          "number": 2,
          "difficulty": "Unknown",
          "type": "multiple_select",
          "question": "**(Medium) ** What are the key parameters set for the TfidfVectorizer in this demo?",
          "options": {
            "A": "`token_pattern = r'\\b[a-zA-Z]{3,}\\b'`",
            "B": "`stop_words` includes custom tokens like \"xxxx\"",
            "C": "`ngram_range = (1,2)`",
            "D": "`max_features = 1000`",
            "E": "`use_idf = False`"
          }
        },
        {
          "number": 3,
          "difficulty": "Medium",
          "type": "single_select",
          "question": "Why is it acceptable to use TF-IDF weighting for clustering but not for LDA topic modeling?",
          "options": {
            "A": "Clustering only works with binary features",
            "B": "LDA requires nonzero word counts for probabilistic modeling",
            "C": "TF-IDF always improves LDA",
            "D": "Clustering algorithms do not require integer counts"
          }
        },
        {
          "number": 4,
          "difficulty": "Unknown",
          "type": "multiple_select",
          "question": "**(Hard) ** What are the main challenges of clustering high-dimensional textual data?",
          "options": {
            "A": "Lack of stopword removal",
            "B": "Sparsity of data",
            "C": "Scale differences between features",
            "D": "Curse of dimensionality"
          }
        },
        {
          "number": 5,
          "difficulty": "Hard",
          "type": "single_select",
          "question": "What is the effect of standardizing features before clustering?",
          "options": {
            "A": "The features are become more interpretable and accurate",
            "B": "All features are converted to binary",
            "C": "Each feature has mean zero and standard deviation one",
            "D": "The DTM becomes denser"
          }
        }
      ]
    },
    {
      "title": "Section 2: K-means Clustering and Cluster Evaluation",
      "questions": [
        {
          "number": 6,
          "difficulty": "Easy",
          "type": "single_select",
          "question": "What is the primary parameter that must be set when fitting a K-means model?",
          "options": {
            "A": "Distance metric",
            "B": "Number of clusters (`n_clusters`)",
            "C": "Number of features",
            "D": "Batch size"
          }
        },
        {
          "number": 7,
          "difficulty": "Unknown",
          "type": "multiple_select",
          "question": "**(Medium) ** What are typical ways to evaluate the distribution of cluster assignments in K-means?",
          "options": {
            "A": "value_counts() on cluster labels",
            "B": "Reviewing the number of points per cluster",
            "C": "Examining the most and least populous clusters",
            "D": "Counting the number of unique words"
          }
        },
        {
          "number": 8,
          "difficulty": "Medium",
          "type": "single_select",
          "question": "What does the silhouette score measure in clustering evaluation?",
          "options": {
            "A": "The average similarity between each cluster and its most similar cluster",
            "B": "The accuracy of each point assignment in each cluster",
            "C": "The within-cluster sum of squares",
            "D": "How well each point fits within its assigned cluster compared to others"
          }
        },
        {
          "number": 9,
          "difficulty": "Unknown",
          "type": "multiple_select",
          "question": "**(Hard) ** Which methods are used to interpret the meaning of clusters in K-means?",
          "options": {
            "A": "Examining most prevalent words in each cluster",
            "B": "Identifying features most dissimilar from other clusters",
            "C": "Finding the document closest to each cluster center",
            "D": "Counting the number of clusters"
          }
        },
        {
          "number": 10,
          "difficulty": "Hard",
          "type": "single_select",
          "question": "Why is it useful to standardize cluster centers before identifying influential words?",
          "options": {
            "A": "To compare feature importance across clusters on a common scale",
            "B": "To remove all negative values",
            "C": "To convert all features to binary",
            "D": "To increase the number of clusters"
          }
        }
      ]
    },
    {
      "title": "Section 3: Dimension Reduction and PCA",
      "questions": [
        {
          "number": 11,
          "difficulty": "Easy",
          "type": "single_select",
          "question": "What is the primary purpose of applying PCA before clustering?",
          "options": {
            "A": "Makes the data more sparse and therefore increases computational efficiency",
            "B": "Selects the most relevant features for analysis",
            "C": "Reduce the number of features while retaining most variance",
            "D": "Normalize the DTM"
          }
        },
        {
          "number": 12,
          "difficulty": "Unknown",
          "type": "multiple_select",
          "question": "**(Medium) ** What are common criteria for selecting the number of principal components to retain in PCA?",
          "options": {
            "A": "Eigenvalues greater than 1",
            "B": "Cumulative explained variance threshold (e.g., 75%)",
            "C": "Minimum word frequency",
            "D": "Fixed number of components"
          }
        },
        {
          "number": 13,
          "difficulty": "Medium",
          "type": "single_select",
          "question": "What is the effect of standardizing the PCA-transformed matrix before clustering?",
          "options": {
            "A": "All features are converted to binary",
            "B": "The number of clusters is fixed",
            "C": "Each component has mean zero and standard deviation one",
            "D": "The DTM becomes sparser"
          }
        },
        {
          "number": 14,
          "difficulty": "Unknown",
          "type": "multiple_select",
          "question": "**(Hard) ** What information can be obtained from the `explained_variance_ratio_` attribute in sklearn’s PCA?",
          "options": {
            "A": "Proportion of variance explained by each component",
            "B": "Cumulative variance explained",
            "C": "Which components to retain",
            "D": "The number of clusters"
          }
        },
        {
          "number": 15,
          "difficulty": "Hard",
          "type": "single_select",
          "question": "Why might you want to retain principal components explaining up to 75% of variance?",
          "options": {
            "A": "To reduce the size of data by 75%",
            "B": "To balance dimensionality reduction with information retention",
            "C": "To maximize the number of clusters",
            "D": "To remove all rare words"
          }
        }
      ]
    },
    {
      "title": "Section 4: HDBSCAN, UMAP, and Advanced Clustering",
      "questions": [
        {
          "number": 16,
          "difficulty": "Easy",
          "type": "single_select",
          "question": "What does HDBSCAN label as “-1” in its cluster assignments?",
          "options": {
            "A": "The last cluster",
            "B": "The largest cluster",
            "C": "Outliers only in PCA",
            "D": "Noise points"
          }
        },
        {
          "number": 17,
          "difficulty": "Unknown",
          "type": "multiple_select",
          "question": "**(Medium) ** What are the key parameters for tuning HDBSCAN?",
          "options": {
            "A": "cluster_selection_method",
            "B": "min_cluster_size",
            "C": "min_samples",
            "D": "cluster_selection_epsilon",
            "E": "n_init"
          }
        },
        {
          "number": 18,
          "difficulty": "Medium",
          "type": "single_select",
          "question": "Why does HDBSCAN often classify a large percentage of points as noise in high-dimensional data?",
          "options": {
            "A": "HDBSCAN only works with binary features",
            "B": "High-dimensional data is sparse, so many points appear unique",
            "C": "It requires integer counts",
            "D": "It always produces balanced clusters"
          }
        },
        {
          "number": 19,
          "difficulty": "Unknown",
          "type": "multiple_select",
          "question": "**(Hard) ** What are recommended strategies for evaluating HDBSCAN results?",
          "options": {
            "A": "Proportion of points classified as noise",
            "B": "Number of clusters",
            "C": "Maximum and standard deviation of cluster sizes",
            "D": "Number of clusters at minimum size"
          }
        },
        {
          "number": 20,
          "difficulty": "Hard",
          "type": "single_select",
          "question": "Why is UMAP often preferred over PCA for dimensionality reduction before density-based clustering?",
          "options": {
            "A": "UMAP preserves local structure and relationships better",
            "B": "UMAP always produces fewer clusters",
            "C": "PCA is not supported in sklearn",
            "D": "UMAP removes all outliers"
          }
        }
      ]
    }
  ]
}