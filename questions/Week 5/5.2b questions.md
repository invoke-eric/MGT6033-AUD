# Quiz Questions

## Section 1: Latent Dirichlet Allocation (LDA) Concepts

1. **(Easy)** What does the "latent" in Latent Dirichlet Allocation (LDA) refer to?  
A. Observed word counts  
B. Hidden or implicit topics  
C. Document length  
D. The number of documents  

2. **(Medium, Select all that apply):** Which of the following are key assumptions of the LDA generative process?  
A. Each word occurs with some probability within each topic  
B. Each topic occurs with some probability throughout each document  
C. Each document contains only one topic  
D. Word and topic distributions are governed by Dirichlet priors  

3. **(Medium)** What is the main role of the Dirichlet distribution in LDA?  
A. It is used to generate observed words  
B. It acts as a prior for topic and word distributions  
C. It determines the number of documents  
D. It is used for evaluating model fit  

4. **(Hard, Select all that apply):** Which statements about the Alpha and Beta hyperparameters in LDA are correct?  
A. Alpha is the topic prior, controlling topic distribution within documents  
B. Beta is the word prior, controlling word distribution within topics  
C. Higher Alpha leads to documents with more uniform topic distributions  
D. Lower Beta leads to topics with more uniform word distributions  

5. **(Hard)** Why can't you use TF-IDF as input to LDA?  
A. LDA requires binary input  
B. LDA requires integer word counts  
C. LDA only works with normalized data  
D. LDA requires document labels  

## Section 2: Non-negative Matrix Factorization (NMF) Concepts

6. **(Easy)** What does "non-negative" refer to in Non-negative Matrix Factorization (NMF)?  
A. The matrix contains only positive or zero values  
B. The matrix contains only negative values  
C. The matrix is always square  
D. The matrix contains only binary values  

7. **(Medium, Select all that apply):** In NMF, which matrices are produced by factorizing the document-term matrix?  
A. Document-topic matrix (W)  
B. Topic-word matrix (H)  
C. Word-label matrix (L)  
D. Document-frequency matrix (F)  

8. **(Medium)** What is the main intuition behind NMF for topic modeling?  
A. Decompose the document-term matrix into two smaller, interpretable matrices  
B. Assign each document to a single topic  
C. Use Bayesian priors to generate topics  
D. Normalize all word counts to probabilities  

9. **(Hard, Select all that apply):** Which of the following are true about the outputs of LDA and NMF?  
A. Both produce a topic-word matrix  
B. Both produce a document-topic matrix  
C. Both require the number of topics to be specified by the user  
D. Both guarantee identical results for the same data  

10. **(Hard)** How do LDA and NMF differ in their underlying approach to topic modeling?  
A. LDA is Bayesian and generative, NMF is purely mathematical and factorizes the DTM  
B. Both use Dirichlet distributions for priors  
C. NMF requires labeled data, LDA does not  
D. LDA only works with binary matrices  

