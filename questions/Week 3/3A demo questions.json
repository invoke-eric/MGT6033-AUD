{
  "title": "Quiz Questions",
  "sections": [
    {
      "title": "Section 1: Loading and Tokenizing Text",
      "questions": [
        {
          "number": 1,
          "difficulty": "Easy",
          "type": "single_select",
          "question": "Which Python method is commonly used to split a string into tokens based on whitespace?",
          "options": {
            "A": "split()",
            "B": "partition()",
            "C": "find()",
            "D": "groupby()"
          }
        },
        {
          "number": 2,
          "difficulty": "Unknown",
          "type": "multiple_select",
          "question": "**(Medium) ** Which of the following statements about using `word_tokenize` from NLTK are true?",
          "options": {
            "A": "Handles punctuation more accurately than basic split",
            "B": "Returns a list of tokens",
            "C": "Must always be used on lowercase strings",
            "D": "Converts numeric values to words by default"
          }
        },
        {
          "number": 3,
          "difficulty": "Medium",
          "type": "single_select",
          "question": "What is the main reason to filter out stop words when analyzing tokenized text?",
          "options": {
            "A": "To increase the number of tokens",
            "B": "To remove semantically low-value words that are very common",
            "C": "To preserve proper nouns",
            "D": "To make all tokens lowercase"
          }
        },
        {
          "number": 4,
          "difficulty": "Unknown",
          "type": "multiple_select",
          "question": "**(Hard) ** When processing text for NLP, what are important steps before analysis?",
          "options": {
            "A": "Tokenization",
            "B": "Lemmatization or stemming",
            "C": "Removing stop words",
            "D": "Sorting tokens alphabetically"
          }
        }
      ]
    },
    {
      "title": "Section 2: Stemming vs. Lemmatization",
      "questions": [
        {
          "number": 5,
          "difficulty": "Medium",
          "type": "single_select",
          "question": "What attribute of a spaCy token object returns the lemma (root form) of a word?",
          "options": {
            "A": ".stem_",
            "B": ".lemma_",
            "C": ".root_",
            "D": ".text_"
          }
        },
        {
          "number": 6,
          "difficulty": "Hard",
          "type": "single_select",
          "question": "Why might the lemma of a word appear identical to its original text in a preprocessed file?",
          "options": {
            "A": "The file has already been lemmatized",
            "B": "The word is a stop word",
            "C": "Stemming was used instead of lemmatization",
            "D": "All spaCy Doc objects return the same value for text and lemma"
          }
        },
        {
          "number": 7,
          "difficulty": "Unknown",
          "type": "multiple_select",
          "question": "**(Hard) ** What differences exist between Porter and Snowball stemmers in NLTK?",
          "options": {
            "A": "They may produce different stems for the same word",
            "B": "Snowball requires you to specify the language",
            "C": "Porter is the only one that works on adjectives",
            "D": "Both always return valid English words"
          }
        }
      ]
    },
    {
      "title": "Section 3: Sentence and Token Filtering",
      "questions": [
        {
          "number": 8,
          "difficulty": "Unknown",
          "type": "multiple_select",
          "question": "**(Medium) ** Why might you skip sentences with fewer than 5 tokens in preprocessing?",
          "options": {
            "A": "Such sentences are often headings or non-informative",
            "B": "They improve overall sentence readability",
            "C": "To minimize inclusion of noise like titles or blank lines",
            "D": "To guarantee that only questions are analyzed"
          }
        },
        {
          "number": 9,
          "difficulty": "Hard",
          "type": "single_select",
          "question": "In spaCy, which attributes can be used to filter out punctuation and ensure only clean words are collected?",
          "options": {
            "A": "word.is_alpha",
            "B": "word.is_stop",
            "C": "word.text.isdigit()",
            "D": "word.is_title"
          }
        }
      ]
    },
    {
      "title": "Section 4: Advanced Cleaning and Inspection",
      "questions": [
        {
          "number": 10,
          "difficulty": "Unknown",
          "type": "multiple_select",
          "question": "**(Medium) ** According to the demo, which steps are useful for inspecting and refining a cleaned word list?",
          "options": {
            "A": "Convert non-acronym words to lowercase",
            "B": "Keep acronyms (e.g., NLP) in uppercase",
            "C": "Use Pandas Series to count word frequency",
            "D": "Discard proper nouns entirely"
          }
        },
        {
          "number": 11,
          "difficulty": "Hard",
          "type": "single_select",
          "question": "What is a practical pandas method to examine the most frequent non-stop words in a processed corpus?",
          "options": {
            "A": "groupby()",
            "B": "value_counts()",
            "C": "describe()",
            "D": "get_text()"
          }
        },
        {
          "number": 12,
          "difficulty": "Unknown",
          "type": "multiple_select",
          "question": "**(Hard) ** When iterating over spaCy `sent` and `word` objects, which of the following practices are recommended for high-quality cleaning?",
          "options": {
            "A": "Skip sentences that are too short",
            "B": "Filter out non-alphabetic tokens",
            "C": "Remove stop words",
            "D": "Store all tokens as title-case"
          }
        }
      ]
    },
    {
      "title": "Section 5: Case Sensitivity and Acronym Preservation",
      "questions": [
        {
          "number": 13,
          "difficulty": "Hard",
          "type": "single_select",
          "question": "In the acronym-preserving version of the cleaning loop, what check ensures a token is an acronym?",
          "options": {
            "A": "word.is_upper",
            "B": "word.is_stop",
            "C": "word.is_title",
            "D": "word.text.isnumeric()"
          }
        },
        {
          "number": 14,
          "difficulty": "Unknown",
          "type": "multiple_select",
          "question": "**(Medium) ** Why is it necessary to adjust for casing when tallying frequencies of non-stop words?",
          "options": {
            "A": "To distinguish between singular and plural",
            "B": "To prevent NLP acronyms from being merged with regular words",
            "C": "To make word frequency counts case-insensitive for non-acronyms",
            "D": "To make all data uppercase"
          }
        },
        {
          "number": 15,
          "difficulty": "Unknown",
          "type": "multiple_select",
          "question": "**(Hard) ** What potential consequences might arise if you do not handle acronyms and casing correctly in frequency analysis?",
          "options": {
            "A": "Acronyms may be undercounted",
            "B": "Frequencies for \"NLP\" and \"nlp\" could be split",
            "C": "Case-sensitive duplicates may appear in results",
            "D": "Lemmatization will always fail"
          }
        }
      ]
    }
  ]
}