# Quiz Questions

## Section 1: Fundamentals of Word2Vec and Word Embeddings

1. **(Easy)** What is the main idea behind the Word2Vec algorithm?  
A. To summarize documents into a single number  
B. To learn dense vector representations for words based on their context  
C. To count word occurrences in documents  
D. To label parts of speech in text  

2. **(Medium) Select all that apply:** Which two model architectures are provided by Word2Vec?  
A. Skip-gram  
B. Continuous Bag of Words (CBOW)  
C. Latent Dirichlet Allocation (LDA)  
D. FastText  

3. **(Medium)** What does it mean when we say word embeddings are “dense” vectors?  
A. They contain mostly zeros  
B. They are short vectors with mostly non-zero values  
C. They require enormous storage space  
D. They are only used for images  

4. **(Medium) Select all that apply:** According to the distributional hypothesis, words with similar meanings will...  
A. Appear in similar contexts  
B. Have similar word vectors  
C. Always have the same part of speech  
D. Appear only in the same documents  

5. **(Hard)** Why do word embeddings capture word analogies like “king – man + woman ≈ queen”?  
A. Because embeddings are trained to reflect co-occurrence statistics  
B. Because vector arithmetic in the embedding space encodes semantic relationships  
C. Because all words have the same vector length  
D. Because every embedding is assigned randomly  

## Section 2: Training and Using Word Embeddings

6. **(Medium)** What is the core difference between Skip-gram and CBOW?  
A. Skip-gram predicts context words from a target word; CBOW predicts a target word from context words  
B. Skip-gram uses sparse vectors; CBOW uses dense vectors  
C. CBOW requires no training data  
D. CBOW predicts document topics  

7. **(Medium) Select all that apply:** For which tasks are pre-trained word embeddings especially useful?  
A. Text classification  
B. Named Entity Recognition  
C. Part-of-Speech tagging  
D. Image segmentation  

8. **(Hard)** What is meant by “out-of-vocabulary” (OOV) words in the context of word embeddings?  
A. Words for which there is no trained embedding vector  
B. Words that appear most frequently  
C. Words that always appear at the start of documents  
D. Words with more than ten letters  

9. **(Hard) Select all that apply:** What challenges can arise when using static word embeddings like Word2Vec?  
A. They use the same vector for all senses of a word  
B. They cannot represent polysemy (multiple meanings for one word)  
C. They need to be retrained for every new document  
D. They struggle with rare or unseen terms  

10. **(Hard)** How does cosine similarity help in evaluating relationships between word embeddings?  
A. It measures the angle between two word vectors  
B. High cosine similarity indicates semantic similarity  
C. It identifies stop words efficiently  
D. It sorts embeddings by frequency  

