{
  "title": "Quiz Questions",
  "sections": [
    {
      "title": "Section 1: Fundamentals of Word2Vec and Word Embeddings",
      "questions": [
        {
          "number": 1,
          "difficulty": "Easy",
          "type": "single_select",
          "question": "What is the main idea behind the Word2Vec algorithm?",
          "options": {
            "A": "To summarize documents into a single number",
            "B": "To learn dense vector representations for words based on their context",
            "C": "To count word occurrences in documents",
            "D": "To label parts of speech in text"
          }
        },
        {
          "number": 2,
          "difficulty": "Unknown",
          "type": "multiple_select",
          "question": "**(Medium) ** Which two model architectures are provided by Word2Vec?",
          "options": {
            "A": "Skip-gram",
            "B": "Continuous Bag of Words (CBOW)",
            "C": "Latent Dirichlet Allocation (LDA)",
            "D": "FastText"
          }
        },
        {
          "number": 3,
          "difficulty": "Medium",
          "type": "single_select",
          "question": "What does it mean when we say word embeddings are “dense” vectors?",
          "options": {
            "A": "They contain mostly zeros",
            "B": "They are short vectors with mostly non-zero values",
            "C": "They require enormous storage space",
            "D": "They are only used for images"
          }
        },
        {
          "number": 4,
          "difficulty": "Unknown",
          "type": "multiple_select",
          "question": "**(Medium) ** According to the distributional hypothesis, words with similar meanings will...",
          "options": {
            "A": "Appear in similar contexts",
            "B": "Have similar word vectors",
            "C": "Always have the same part of speech",
            "D": "Appear only in the same documents"
          }
        },
        {
          "number": 5,
          "difficulty": "Hard",
          "type": "single_select",
          "question": "Why do word embeddings capture word analogies like “king – man + woman ≈ queen”?",
          "options": {
            "A": "Because embeddings are trained to reflect co-occurrence statistics",
            "B": "Because vector arithmetic in the embedding space encodes semantic relationships",
            "C": "Because all words have the same vector length",
            "D": "Because every embedding is assigned randomly"
          }
        }
      ]
    },
    {
      "title": "Section 2: Training and Using Word Embeddings",
      "questions": [
        {
          "number": 6,
          "difficulty": "Medium",
          "type": "single_select",
          "question": "What is the core difference between Skip-gram and CBOW?",
          "options": {
            "A": "Skip-gram predicts context words from a target word; CBOW predicts a target word from context words",
            "B": "Skip-gram uses sparse vectors; CBOW uses dense vectors",
            "C": "CBOW requires no training data",
            "D": "CBOW predicts document topics"
          }
        },
        {
          "number": 7,
          "difficulty": "Unknown",
          "type": "multiple_select",
          "question": "**(Medium) ** For which tasks are pre-trained word embeddings especially useful?",
          "options": {
            "A": "Text classification",
            "B": "Named Entity Recognition",
            "C": "Part-of-Speech tagging",
            "D": "Image segmentation"
          }
        },
        {
          "number": 8,
          "difficulty": "Hard",
          "type": "single_select",
          "question": "What is meant by “out-of-vocabulary” (OOV) words in the context of word embeddings?",
          "options": {
            "A": "Words for which there is no trained embedding vector",
            "B": "Words that appear most frequently",
            "C": "Words that always appear at the start of documents",
            "D": "Words with more than ten letters"
          }
        },
        {
          "number": 9,
          "difficulty": "Unknown",
          "type": "multiple_select",
          "question": "**(Hard) ** What challenges can arise when using static word embeddings like Word2Vec?",
          "options": {
            "A": "They use the same vector for all senses of a word",
            "B": "They cannot represent polysemy (multiple meanings for one word)",
            "C": "They need to be retrained for every new document",
            "D": "They struggle with rare or unseen terms"
          }
        },
        {
          "number": 10,
          "difficulty": "Hard",
          "type": "single_select",
          "question": "How does cosine similarity help in evaluating relationships between word embeddings?",
          "options": {
            "A": "It measures the angle between two word vectors",
            "B": "High cosine similarity indicates semantic similarity",
            "C": "It identifies stop words efficiently",
            "D": "It sorts embeddings by frequency"
          }
        }
      ]
    }
  ]
}