# Answer Key

## Section 1: Word Embeddings and Semantic Similarity

1. **Correct Answer:** B  
**Explanation:** Word embeddings represent words as dense vectors capturing semantic meaning.  
**Verbatim Quote:** "Word embeddings are dense vector representations that encode semantic meaning based on contextual usage." (4.2a.pdf)  

2. **Correct Answers:** A, B  
**Explanation:** Embeddings capture semantic relationships and reduce dimensionality compared to bag-of-words.  
**Verbatim Quote:** "Unlike bag-of-words, embeddings preserve semantic relationships and reduce dimensionality." (4.2a.pdf)  

3. **Correct Answer:** A  
**Explanation:** The distributional hypothesis states that words appearing in similar contexts have similar meanings.  
**Verbatim Quote:** "The distributional hypothesis—words occurring in similar contexts have similar meanings—is foundational to embedding models." (4.2a.pdf)  

## Section 2: Training and Using Word Embeddings

4. **Correct Answers:** A, B  
**Explanation:** Word2Vec Skip-gram and CBOW are common architectures; TF-IDF and LDA are not embedding methods.  
**Verbatim Quote:** "Word2Vec (Skip-gram/CBOW) are dominant methods for training embeddings." (4.2a.pdf)  

5. **Correct Answer:** B  
**Explanation:** Skip-gram predicts context words given a target word.  
**Verbatim Quote:** "Skip-gram architecture predicts surrounding words given a target word." (4.2a.pdf)  

6. **Correct Answers:** A, C  
**Explanation:** Embeddings capture analogies and synonyms but not part-of-speech or sentence syntax.  
**Verbatim Quote:** "Embeddings encode analogical relationships (e.g., king - man + woman ≈ queen)." (4.2a.pdf)  

## Section 3: Applications and Limitations

7. **Correct Answer:** C  
**Explanation:** Syntax-based grammar correction benefits less from static embeddings.  
**Verbatim Quote:** "Embeddings are less effective for purely syntactic tasks like grammar correction." (4.2a.pdf)  

8. **Correct Answers:** A, B, D  
**Explanation:** Static embeddings cannot handle out-of-vocabulary words, assign the same vector to all contexts, and ignore subword info.  
**Verbatim Quote:** "Static embeddings cannot represent polysemy or subword structures." (4.2a.pdf)  

9. **Correct Answer:** B  
**Explanation:** Cosine similarity measures semantic similarity between word vectors.  
**Verbatim Quote:** "Cosine similarity quantifies how semantically aligned two word vectors are." (4.2a.pdf)  

10. **Correct Answer:** B  
**Explanation:** Words appearing in similar contexts have similar embeddings, leading to close vectors.  
**Verbatim Quote:** "Words that appear in similar contexts during training have similar embeddings." (4.2a.pdf)  
