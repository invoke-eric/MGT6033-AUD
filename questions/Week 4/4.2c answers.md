# Answer Key

## Section 1: Word Embeddings â€“ Use Cases and Interpretability

1. **Correct Answer:** B  
**Explanation:** Word embeddings allow models to capture semantic similarity between words, which is crucial for many NLP tasks.  
**Verbatim Quote:** "Word embeddings are used to capture semantic similarity between words in a dense vector space." (4.2c-Word-Embeddings-Use-Cases.txt)

2. **Correct Answers:** A, B, C  
**Explanation:** Embeddings are widely used in sentiment analysis, document classification, and NER.  
**Verbatim Quote:** "Word embeddings are commonly used in sentiment analysis, document classification, and named entity recognition." (4.2c-Word-Embeddings-Use-Cases.txt)

3. **Correct Answer:** B  
**Explanation:** Embeddings capture relationships; one-hot encoding does not.  
**Verbatim Quote:** "Unlike one-hot encoding, embeddings capture relationships between words." (4.2c-Word-Embeddings-Use-Cases.txt)

4. **Correct Answers:** A, B, D  
**Explanation:** Embedding dimensions are not human-interpretable, but vector arithmetic and dimensionality reduction can reveal structure and analogies.  
**Verbatim Quote:** "Individual embedding dimensions are not interpretable, but vector arithmetic can reveal analogies, and dimensionality reduction can help visualize embeddings." (4.2c-Word-Embeddings-Use-Cases.txt)

5. **Correct Answer:** B  
**Explanation:** Words with similar meanings appear in similar contexts, leading to similar vectors.  
**Verbatim Quote:** "Words that appear in similar contexts during training have similar embeddings." (4.2c-Word-Embeddings-Use-Cases.txt)

## Section 2: Embedding Limitations and Best Practices

6. **Correct Answer:** B  
**Explanation:** Static embeddings assign the same vector to a word regardless of context, which is a key limitation.  
**Verbatim Quote:** "A limitation of static embeddings is that they assign the same vector to a word in all contexts." (4.2c-Word-Embeddings-Use-Cases.txt)

7. **Correct Answers:** A, C, D  
**Explanation:** Best practices include domain adaptation, checking for OOV words, and evaluating performance.  
**Verbatim Quote:** "Best practices: consider domain-specific training, check for OOV words, and evaluate embeddings on your task." (4.2c-Word-Embeddings-Use-Cases.txt)

8. **Correct Answer:** B  
**Explanation:** Dimensionality reduction helps visualize high-dimensional embedding relationships.  
**Verbatim Quote:** "Dimensionality reduction methods like t-SNE or PCA are used to visualize relationships between word embeddings." (4.2c-Word-Embeddings-Use-Cases.txt)

9. **Correct Answers:** A, B, D  
**Explanation:** It is important to ensure vocabulary coverage, model compatibility, and that all words have embeddings.  
**Verbatim Quote:** "Before using pre-trained embeddings, check that the vocabulary matches your data and that all words have vectors." (4.2c-Word-Embeddings-Use-Cases.txt)

10. **Correct Answer:** A  
**Explanation:** If your text domain is highly specialized, general embeddings may not perform well and you may need to train your own.  
**Verbatim Quote:** "If your domain is highly specialized, you may need to train embeddings from scratch." (4.2c-Word-Embeddings-Use-Cases.txt)
