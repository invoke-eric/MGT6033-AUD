{
  "title": "Quiz Questions",
  "sections": [
    {
      "title": "Section 1: BERT and Transfer Learning Fundamentals",
      "questions": [
        {
          "number": 1,
          "difficulty": "Easy",
          "type": "single_select",
          "question": "What does BERT stand for?",
          "options": {
            "A": "Bidirectional Encoder Representations from Transformers",
            "B": "Binary Encoder Recurrent Transformer",
            "C": "Basic Entity Recognition Transformer",
            "D": "Bidirectional Embedding Regression Transformer"
          }
        },
        {
          "number": 2,
          "difficulty": "Unknown",
          "type": "single_select",
          "question": "**(Medium, Select all that apply):** Which of the following are common applications of BERT as described in the lecture?",
          "options": {
            "A": "Text classification",
            "B": "Masked language modeling",
            "C": "Semantic similarity",
            "D": "Text generation",
            "E": "Image recognition"
          }
        },
        {
          "number": 3,
          "difficulty": "Medium",
          "type": "single_select",
          "question": "What is the primary benefit of transfer learning with models like BERT for NLP tasks?",
          "options": {
            "A": "It allows fine-tuning pre-trained parameters with a small sample of labeled data",
            "B": "It requires no labeled data",
            "C": "It eliminates the need for embeddings",
            "D": "It guarantees perfect accuracy"
          }
        }
      ]
    },
    {
      "title": "Section 2: BERT Model Applications and Design",
      "questions": [
        {
          "number": 4,
          "difficulty": "Unknown",
          "type": "single_select",
          "question": "**(Hard, Select all that apply):** When designing a sentiment classifier using BERT, what steps are involved?",
          "options": {
            "A": "Add a classification head to the model",
            "B": "Fine-tune with labeled data for the target task",
            "C": "Train BERT from scratch on your data",
            "D": "Use BERT’s output embeddings for classification"
          }
        },
        {
          "number": 5,
          "difficulty": "Medium",
          "type": "single_select",
          "question": "For which of the following tasks is BERT’s masked language modeling especially useful?",
          "options": {
            "A": "Predicting missing words in a sentence",
            "B": "Counting word frequency",
            "C": "Removing stop words",
            "D": "Sorting documents alphabetically"
          }
        },
        {
          "number": 6,
          "difficulty": "Unknown",
          "type": "single_select",
          "question": "**(Hard, Select all that apply):** What are some practical uses of masked language modeling with BERT?",
          "options": {
            "A": "Measuring entropy or \"new information\" in a document",
            "B": "Assessing text or grammar quality",
            "C": "Identifying anomalous documents",
            "D": "Generating word clouds"
          }
        },
        {
          "number": 7,
          "difficulty": "Hard",
          "type": "single_select",
          "question": "What is a well-known limitation of masked language models like BERT, as discussed in the lecture?",
          "options": {
            "A": "They are always unbiased",
            "B": "They can perpetuate biases present in training data",
            "C": "They require huge labeled datasets for every task",
            "D": "They cannot be fine-tuned"
          }
        }
      ]
    },
    {
      "title": "Section 3: Advanced Applications and Considerations",
      "questions": [
        {
          "number": 8,
          "difficulty": "Medium",
          "type": "single_select",
          "question": "How does BERT measure semantic similarity between documents?",
          "options": {
            "A": "By comparing contextualized embeddings generated by the encoder",
            "B": "By counting shared words",
            "C": "By matching document lengths",
            "D": "By using only the first sentence"
          }
        },
        {
          "number": 9,
          "difficulty": "Unknown",
          "type": "single_select",
          "question": "**(Hard, Select all that apply):** Which of the following are examples of advanced applications or extensions of BERT?",
          "options": {
            "A": "BERTopic for topic modeling",
            "B": "Automatic summarization",
            "C": "Plagiarism detection",
            "D": "Image segmentation"
          }
        },
        {
          "number": 10,
          "difficulty": "Hard",
          "type": "single_select",
          "question": "What is a key consideration when using generative models like BERT or GPT for text generation?",
          "options": {
            "A": "They are only as good as the text on which they are trained",
            "B": "They always provide expert-level answers",
            "C": "They require no fine-tuning",
            "D": "They can only generate single-word responses"
          }
        }
      ]
    }
  ]
}