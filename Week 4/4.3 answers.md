# Answer Key

## Section 1: Contextualized Word Embeddings

1. **Correct Answer:** B  
**Explanation:** Contextualized embeddings change based on the surrounding words.  
**Verbatim Quote:** "A contextualized embedding is a word representation that varies depending on the context in which the word appears." (4.3-Contextualized-Embeddings.txt)

2. **Correct Answers:** A, B, D  
**Explanation:** Contextualized embeddings can represent polysemy, are sensitive to context, and improve performance on polysemous tasks.  
**Verbatim Quote:** "Contextualized embeddings can represent multiple meanings...are sensitive to sentence context...improve performance on polysemous words." (4.3-Contextualized-Embeddings.txt)

3. **Correct Answer:** B  
**Explanation:** Static embeddings fail when a word has multiple meanings; contextualized embeddings succeed by adapting to context.  
**Verbatim Quote:** "Static embeddings assign the same vector to a word regardless of context, while contextualized embeddings adapt to each usage." (4.3-Contextualized-Embeddings.txt)

4. **Correct Answers:** B, C  
**Explanation:** ELMo and BERT are contextualized models; Word2Vec and GloVe are static.  
**Verbatim Quote:** "Popular contextualized embedding models include ELMo and BERT." (4.3-Contextualized-Embeddings.txt)

5. **Correct Answer:** B  
**Explanation:** Contextualized embeddings encode word meaning based on full sentence context, which is critical for complex tasks.  
**Verbatim Quote:** "They encode the meaning of a word based on its context in the sentence." (4.3-Contextualized-Embeddings.txt)

## Section 2: Technical Details and Applications

6. **Correct Answer:** B  
**Explanation:** BERT uses transformer layers to encode the entire sentence context for each word.  
**Verbatim Quote:** "BERT uses transformer layers to encode context from the entire sentence." (4.3-Contextualized-Embeddings.txt)

7. **Correct Answers:** A, B, C  
**Explanation:** Contextualized embeddings are used in NER, sentiment analysis, and machine translation, but not image classification.  
**Verbatim Quote:** "Applications include NER, sentiment analysis, and machine translation." (4.3-Contextualized-Embeddings.txt)

8. **Correct Answer:** A  
**Explanation:** Contextualized embeddings require running a deep model for each sentence, increasing computational cost.  
**Verbatim Quote:** "They are more computationally expensive because a deep model must be run for each new sentence." (4.3-Contextualized-Embeddings.txt)

9. **Correct Answers:** A, B, C  
**Explanation:** Challenges include higher computational cost, need for GPUs, and larger models.  
**Verbatim Quote:** "Challenges: increased computational cost, GPU requirements, and larger model sizes." (4.3-Contextualized-Embeddings.txt)

10. **Correct Answer:** B  
**Explanation:** Contextualized embeddings improve similarity by representing words based on their sentence context.  
**Verbatim Quote:** "By representing each word based on its sentence context, contextualized embeddings can improve document similarity measures." (4.3-Contextualized-Embeddings.txt)
