# Quiz Questions

## Section 1: Contextualized Word Embeddings

1. **(Easy)** What is a contextualized word embedding?  
A. A fixed vector for each word in the vocabulary  
B. A word representation that varies depending on the surrounding words  
C. A one-hot encoded vector  
D. A random vector assigned to each word  

2. **(Medium) Select all that apply:** Which of the following are advantages of contextualized embeddings compared to static embeddings?  
A. They can represent multiple meanings of the same word  
B. They are sensitive to sentence context  
C. They assign the same vector to a word in all contexts  
D. They improve performance on tasks involving polysemy  

3. **(Medium)** In which scenario would a static embedding fail but a contextualized embedding succeed?  
A. When a word has only one meaning  
B. When a word has multiple meanings depending on context  
C. When all sentences are identical  
D. When the vocabulary is very small  

4. **(Medium) Select all that apply:** Which models are commonly used to generate contextualized word embeddings?  
A. Word2Vec  
B. ELMo  
C. BERT  
D. GloVe  

5. **(Hard)** What property allows contextualized embeddings to outperform static embeddings in tasks like question answering?  
A. They use more memory  
B. They encode the meaning of a word based on its context in the sentence  
C. They ignore word order  
D. They are always lower dimensional  

## Section 2: Technical Details and Applications

6. **(Medium)** How does BERT generate a contextualized embedding for a word?  
A. By averaging all word vectors in the vocabulary  
B. By using transformer layers to encode the entire sentence context  
C. By assigning a random vector to each word  
D. By using only the previous word as context  

7. **(Hard) Select all that apply:** What are typical applications of contextualized word embeddings?  
A. Named Entity Recognition (NER)  
B. Sentiment analysis  
C. Machine translation  
D. Image classification  

8. **(Hard)** Why are contextualized embeddings more computationally expensive than static embeddings?  
A. They require running a deep model for each new sentence  
B. They store more vectors per word  
C. They use sparse vectors  
D. They do not use neural networks  

9. **(Hard) Select all that apply:** What challenges are associated with using contextualized embeddings in production systems?  
A. Increased computational cost  
B. Need for GPU acceleration  
C. Larger model sizes  
D. Inability to handle polysemy  

10. **(Hard)** How can contextualized embeddings be used to improve document similarity measures?  
A. By averaging static word vectors  
B. By representing each word based on its sentence context, leading to more accurate similarity scores  
C. By ignoring all context  
D. By using only the first word in each document  

