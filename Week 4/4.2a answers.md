# Answer Key

## Section 1: Word Embedding Fundamentals

1. **Correct Answer:** B  
**Explanation:** Word embeddings map words to vectors that capture semantic relationships.  
**Verbatim Quote:** "Word embeddings are dense vector representations that encode semantic meaning based on contextual usage." (4.1-Word-Embeddings.pdf)  

2. **Correct Answers:** A, B  
**Explanation:** Embeddings capture semantics and reduce dimensionality compared to sparse one-hot encoding.  
**Verbatim Quote:** "Unlike bag-of-words, embeddings preserve semantic relationships and reduce dimensionality." (4.1-Word-Embeddings.pdf)  

3. **Correct Answer:** A  
**Explanation:** The distributional hypothesis states that contextually similar words have similar meanings.  
**Verbatim Quote:** "The distributional hypothesis—words occurring in similar contexts have similar meanings—is foundational to embedding models." (4.1-Word-Embeddings.pdf)  

## Section 2: Training and Properties

4. **Correct Answers:** A, B  
**Explanation:** Word2Vec and GloVe are embedding-specific methods; TF-IDF and LDA are not.  
**Verbatim Quote:** "Word2Vec (Skip-gram/CBOW) and GloVe are dominant methods for training embeddings." (4.2-Training-Embeddings.pdf)  

5. **Correct Answer:** B  
**Explanation:** Skip-gram predicts context words from a target word.  
**Verbatim Quote:** "Skip-gram architecture predicts surrounding words given a target word." (4.2-Training-Embeddings.pdf)  

6. **Correct Answers:** A, C  
**Explanation:** Embeddings capture analogies and synonyms but not syntax or POS tags.  
**Verbatim Quote:** "Embeddings encode analogical relationships (e.g., king - man + woman ≈ queen)." (4.3-Embedding-Properties.pdf)  

7. **Correct Answer:** B  
**Explanation:** Lower dimensions trade nuance for efficiency; higher dimensions may overfit.  
**Verbatim Quote:** "Optimal dimensionality balances computational efficiency and semantic granularity." (4.3-Embedding-Properties.pdf)  

## Section 3: Applications and Evaluation

8. **Correct Answer:** C  
**Explanation:** Syntax-based tasks (e.g., grammar correction) rely less on semantic embeddings.  
**Verbatim Quote:** "Embeddings are less effective for purely syntactic tasks like grammar correction." (4.4-Embedding-Applications.pdf)  

9. **Correct Answers:** A, B, D  
**Explanation:** Static embeddings fail with OOV words, ignore context, and lack subword info.  
**Verbatim Quote:** "Static embeddings cannot represent polysemy or subword structures." (4.5-Embedding-Limitations.pdf)  

10. **Correct Answer:** B  
**Explanation:** Cosine similarity measures semantic closeness between vectors.  
**Verbatim Quote:** "Cosine similarity quantifies how semantically aligned two word vectors are." (4.4-Embedding-Applications.pdf)  
