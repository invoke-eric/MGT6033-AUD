# Answer Key

## Section 1: Fundamentals of Word2Vec and Word Embeddings

1. **Correct Answer:** B  
**Explanation:** Word2Vec learns vector representations for words based on their context within a corpus.  
**Verbatim Quote:** “Word2Vec is a method for learning dense word representations—‘embeddings’—from unstructured text.” (4.2b-Word-Embeddings-Word2Vec.txt)

2. **Correct Answers:** A, B  
**Explanation:** Word2Vec uses the Skip-gram and CBOW model architectures.  
**Verbatim Quote:** “Two main architectures: skip-gram and continuous bag of words (CBOW).” (4.2b-Word-Embeddings-Word2Vec.txt)

3. **Correct Answer:** B  
**Explanation:** “Dense” embeddings are short vectors with mostly non-zero values, unlike sparse representations.  
**Verbatim Quote:** “Embeddings are dense, low-dimensional vectors that capture compact word meaning.” (4.2b-Word-Embeddings-Word2Vec.txt)

4. **Correct Answers:** A, B  
**Explanation:** The distribution hypothesis states similar words occur in similar contexts and have similar embeddings.  
**Verbatim Quote:** “The distributional hypothesis says words found in similar contexts tend to have similar meaning (and representations).” (4.2b-Word-Embeddings-Word2Vec.txt)

5. **Correct Answers:** A, B  
**Explanation:** Embeddings reflect co-occurrences and encode semantic relationships through vector arithmetic.  
**Verbatim Quote:** "Word2Vec’s vector space encodes relationships that allow analogies… because the embeddings reflect how words co-occur.” (4.2b-Word-Embeddings-Word2Vec.txt)

## Section 2: Training and Using Word Embeddings

6. **Correct Answer:** A  
**Explanation:** Skip-gram predicts surrounding words from a target; CBOW predicts a target from context.  
**Verbatim Quote:** “Skip-gram predicts context words given a target. CBOW predicts a target word from a window of context words.” (4.2b-Word-Embeddings-Word2Vec.txt)

7. **Correct Answers:** A, B, C  
**Explanation:** Pre-trained embeddings are widely used in various NLP tasks except for non-text tasks.  
**Verbatim Quote:** “Pre-trained embeddings are useful in tasks like classification, NER, and POS tagging.” (4.2b-Word-Embeddings-Word2Vec.txt)

8. **Correct Answer:** A  
**Explanation:** OOV words are words not present in the embedding’s training vocabulary.  
**Verbatim Quote:** “A major challenge: out-of-vocabulary words—words not seen in training—with no embedding vector.” (4.2b-Word-Embeddings-Word2Vec.txt)

9. **Correct Answers:** A, B, D  
**Explanation:** Static word embeddings use a single vector for all senses and cannot represent unseen words.  
**Verbatim Quote:** “A limitation: static embeddings assign one vector to each word, regardless of context or multiple meanings.” (4.2b-Word-Embeddings-Word2Vec.txt)

10. **Correct Answers:** A, B  
**Explanation:** Cosine similarity measures the angle between vectors; high similarity reflects similar meaning.  
**Verbatim Quote:** “Cosine similarity… quantifies semantic similarity by measuring the angle between word embedding vectors.” (4.2b-Word-Embeddings-Word2Vec.txt)
