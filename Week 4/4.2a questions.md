# Quiz Questions

## Section 1: Word Embedding Fundamentals

1. **(Easy)** What is the primary purpose of word embeddings in NLP?  
A. To count word frequencies  
B. To represent words as dense vectors capturing semantic meaning  
C. To encrypt sensitive text  
D. To split text into sentences  

2. **(Medium) Select all that apply:** Which of the following are key advantages of word embeddings over bag-of-words representations?  
A. They capture semantic relationships between words  
B. They reduce dimensionality compared to one-hot encoding  
C. They ignore word order  
D. They are sparser and more memory-efficient  

3. **(Hard)** What does the term "distributional hypothesis" mean in the context of word embeddings?  
A. Words that appear in similar contexts have similar meanings  
B. Words are distributed randomly in vector space  
C. Embeddings must follow a Gaussian distribution  
D. Word frequencies determine syntactic roles  

## Section 2: Training and Properties

4. **(Medium)** Which of the following are common methods for training word embeddings? (Select all that apply)  
A. Word2Vec (Skip-gram/CBOW)  
B. GloVe  
C. TF-IDF  
D. Latent Dirichlet Allocation (LDA)  

5. **(Hard)** In Word2Vec’s Skip-gram architecture, what is the model trained to predict?  
A. The target word given its context words  
B. The context words given a target word  
C. The document’s topic  
D. The sentiment of a sentence  

6. **(Medium) Select all that apply:** Which properties are typically captured by word embeddings?  
A. Analogies (e.g., king - man + woman ≈ queen)  
B. Part-of-speech tags  
C. Synonyms and antonyms  
D. Sentence-level syntax  

7. **(Hard)** How does the dimensionality of word embeddings affect model performance?  
A. Higher dimensions always improve accuracy  
B. Lower dimensions reduce computational complexity but may lose nuance  
C. Dimensionality has no measurable impact  
D. All embeddings must use 300 dimensions  

## Section 3: Applications and Evaluation

8. **(Medium)** Which NLP task benefits **least** from word embeddings?  
A. Document classification  
B. Sentiment analysis  
C. Syntax-based grammar correction  
D. Named Entity Recognition (NER)  

9. **(Hard) Select all that apply:** What are limitations of static word embeddings (e.g., Word2Vec)?  
A. They cannot handle out-of-vocabulary words  
B. They assign the same vector to a word in all contexts  
C. They require labeled training data  
D. They ignore subword information  

10. **(Hard)** What is the primary purpose of cosine similarity in word embedding applications?  
A. To measure syntactic correctness  
B. To quantify semantic similarity between word vectors  
C. To encrypt text for security  
D. To tokenize sentences  

