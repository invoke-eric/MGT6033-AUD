# Quiz Questions

## Section 1: Word Embeddings and Semantic Similarity

1. **(Easy)** What is the main purpose of word embeddings in NLP?  
A. To count word frequencies  
B. To represent words as dense vectors capturing semantic meaning  
C. To encrypt text data  
D. To split text into sentences  

2. **(Medium) Select all that apply:** Which of the following are advantages of word embeddings over traditional bag-of-words models?  
A. Capture semantic relationships between words  
B. Reduce dimensionality  
C. Ignore word order completely  
D. Are sparse representations  

3. **(Hard)** What is the distributional hypothesis underlying word embeddings?  
A. Words that appear in similar contexts have similar meanings  
B. Words are randomly distributed in text  
C. Word frequency determines meaning  
D. Words with similar spellings have similar meanings  

## Section 2: Training and Using Word Embeddings

4. **(Medium)** Which of the following are common architectures for training word embeddings? (Select all that apply)  
A. Word2Vec Skip-gram  
B. Word2Vec CBOW  
C. TF-IDF  
D. Latent Dirichlet Allocation (LDA)  

5. **(Hard)** In Word2Vec Skip-gram, what is the model trained to predict?  
A. The target word given its context  
B. The context words given a target word  
C. The document topic  
D. The sentiment of a sentence  

6. **(Medium) Select all that apply:** Which properties are typically captured by word embeddings?  
A. Analogies (e.g., king - man + woman â‰ˆ queen)  
B. Part-of-speech tags  
C. Synonyms and antonyms  
D. Sentence-level syntax  

## Section 3: Applications and Limitations

7. **(Easy)** Which NLP task benefits least from static word embeddings?  
A. Document classification  
B. Sentiment analysis  
C. Syntax-based grammar correction  
D. Named Entity Recognition (NER)  

8. **(Hard) Select all that apply:** What are limitations of static word embeddings like Word2Vec?  
A. Cannot handle out-of-vocabulary words  
B. Assign the same vector to a word in all contexts  
C. Require labeled training data  
D. Ignore subword information  

9. **(Medium)** What is the primary use of cosine similarity in word embedding applications?  
A. To measure syntactic correctness  
B. To quantify semantic similarity between word vectors  
C. To encrypt text  
D. To tokenize sentences  

10. **(Hard)** Why might two words with similar meanings have vectors close together in embedding space?  
A. They are spelled similarly  
B. They appear in similar contexts  
C. They have the same frequency  
D. They are both stop words  

