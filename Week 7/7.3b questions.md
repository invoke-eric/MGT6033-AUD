# Quiz Questions

## Section 1: BERT and Transfer Learning Fundamentals

1. **(Easy)** What does BERT stand for?  
A. Bidirectional Encoder Representations from Transformers  
B. Binary Encoder Recurrent Transformer  
C. Basic Entity Recognition Transformer  
D. Bidirectional Embedding Regression Transformer  

2. **(Medium, Select all that apply):** Which of the following are common applications of BERT as described in the lecture?  
A. Text classification  
B. Masked language modeling  
C. Semantic similarity  
D. Text generation  
E. Image recognition  

3. **(Medium)** What is the primary benefit of transfer learning with models like BERT for NLP tasks?  
A. It allows fine-tuning pre-trained parameters with a small sample of labeled data  
B. It requires no labeled data  
C. It eliminates the need for embeddings  
D. It guarantees perfect accuracy  

## Section 2: BERT Model Applications and Design

4. **(Hard, Select all that apply):** When designing a sentiment classifier using BERT, what steps are involved?  
A. Add a classification head to the model  
B. Fine-tune with labeled data for the target task  
C. Train BERT from scratch on your data  
D. Use BERT’s output embeddings for classification  

5. **(Medium)** For which of the following tasks is BERT’s masked language modeling especially useful?  
A. Predicting missing words in a sentence  
B. Counting word frequency  
C. Removing stop words  
D. Sorting documents alphabetically  

6. **(Hard, Select all that apply):** What are some practical uses of masked language modeling with BERT?  
A. Measuring entropy or "new information" in a document  
B. Assessing text or grammar quality  
C. Identifying anomalous documents  
D. Generating word clouds  

7. **(Hard)** What is a well-known limitation of masked language models like BERT, as discussed in the lecture?  
A. They are always unbiased  
B. They can perpetuate biases present in training data  
C. They require huge labeled datasets for every task  
D. They cannot be fine-tuned  

## Section 3: Advanced Applications and Considerations

8. **(Medium)** How does BERT measure semantic similarity between documents?  
A. By comparing contextualized embeddings generated by the encoder  
B. By counting shared words  
C. By matching document lengths  
D. By using only the first sentence  

9. **(Hard, Select all that apply):** Which of the following are examples of advanced applications or extensions of BERT?  
A. BERTopic for topic modeling  
B. Automatic summarization  
C. Plagiarism detection  
D. Image segmentation  

10. **(Hard)** What is a key consideration when using generative models like BERT or GPT for text generation?  
A. They are only as good as the text on which they are trained  
B. They always provide expert-level answers  
C. They require no fine-tuning  
D. They can only generate single-word responses  

---

