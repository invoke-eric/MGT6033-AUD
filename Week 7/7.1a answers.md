# Answer Key

## Section 1: Artificial Neural Network Architecture

1. **Correct Answer:** B  
**Explanation:** The three required elements are input layer, hidden layers, and output layer.  
**Verbatim Quote:** “There are three required elements for an artificial neural network. First, you must have an input layer...Second, we have...hidden layers...Finally, we require an output layer.” (7.1a.-Review-of-ANNs.txt)

2. **Correct Answers:** A, B, C  
**Explanation:** Input layer size is determined by features, hidden layers/nodes are chosen by the user, and output layer depends on the prediction task.  
**Verbatim Quote:** “The input layer is simply determined by the dimensions of our data...Hidden layers...is where things get a bit more exciting. This is completely up to us...The output layer is determined by what the model is predicting.” (7.1a.-Review-of-ANNs.txt)

3. **Correct Answer:** B  
**Explanation:** The activation function transforms the weighted sum of inputs before passing to the next layer.  
**Verbatim Quote:** “the output...is the result of this sum which is transformed by some activation function.” (7.1a.-Review-of-ANNs.txt)

4. **Correct Answers:** A, B, D  
**Explanation:** ReLU, sigmoid, and linear (no activation) are all mentioned as activation functions.  
**Verbatim Quote:** “Those non linear methods are called activation functions...None (simple linear activation)...ReLU...Sigmoid” (7.1a.pdf)

5. **Correct Answer:** A  
**Explanation:** Small networks are less likely to overfit and easier to train; expand as needed for complexity.  
**Verbatim Quote:** “My advice is to start relatively small and then expand the network as needed.” (7.1a.-Review-of-ANNs.txt)

## Section 2: Training and Optimization in ANNs

6. **Correct Answer:** B  
**Explanation:** The forward pass propagates input through the network to compute predictions.  
**Verbatim Quote:** “Next, we're going to complete what's called a forward pass. This is...a pass of data through the model.” (7.1a.-Review-of-ANNs.txt)

7. **Correct Answers:** A, B, C, D  
**Explanation:** All steps—initialize weights, forward pass, compute gradient, backpropagation—are part of standard training.  
**Verbatim Quote:** “1. Initialize weights...2. Forward pass...3. Compute gradient...4. Update weights with backpropagation...Repeat 2-4 until convergence” (7.1a.pdf)

8. **Correct Answer:** B  
**Explanation:** The learning rate controls how much to adjust weights during each update.  
**Verbatim Quote:** “There's also a parameter called a learning rate, which controls how much adjustment we make with each batch.” (7.1a.-Review-of-ANNs.txt)

9. **Correct Answers:** A, B, D  
**Explanation:** Backpropagation uses the gradient to update weights, is repeated until convergence, and minimizes error.  
**Verbatim Quote:** “Using this derivative, we update the weights through a process called back propagation...we repeat these three steps until our weights converge or the maximum number of iterations is reached.” (7.1a.-Review-of-ANNs.txt)

10. **Correct Answer:** A  
**Explanation:** Splitting into training and validation sets helps tune hyperparameters and assess generalization.  
**Verbatim Quote:** “we need to make sure we split the data into training and validation sets, and then we can tune the hyperparameters to find the best design.” (7.1a.-Review-of-ANNs.txt)
