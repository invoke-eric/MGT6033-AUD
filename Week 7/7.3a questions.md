# Quiz Questions

## Section 1: Transformer Architecture Fundamentals

1. **(Easy)** What is the primary learning objective for the lecture on transformers?  
A. To design a transformer from scratch  
B. To discriminate between language transformers and previously employed neural networks  
C. To implement a transformer in Python  
D. To compare transformers to recurrent neural networks  

2. **(Medium, Select all that apply)** Which of the following are key defining characteristics of transformer models?  
A. Self-attention mechanism  
B. Positional encoding  
C. Sequential processing of tokens  
D. Contextual representations  
E. Encoder-decoder architecture  

3. **(Hard)** How does the self-attention mechanism in transformers differ from processing in traditional neural networks?  
A. It processes each token independently  
B. It allows the model to weigh the importance of different words regardless of their position in the sequence  
C. It requires sequential processing of words  
D. It cannot handle long-range dependencies  

4. **(Hard, Select all that apply)** What advantages does the transformer architecture provide over traditional neural networks for NLP tasks?  
A. It enables parallelization of processing  
B. It captures long-range dependencies between words  
C. It requires less training data  
D. It learns context without requiring sequential processing  

5. **(Medium)** What is the role of positional encoding in transformer models?  
A. It helps the model understand sequence order without requiring word-by-word processing  
B. It removes the need for attention mechanisms  
C. It ensures all words are processed sequentially  
D. It replaces the need for contextual representations  

## Section 2: Encoder-Decoder Components

6. **(Hard)** What are the two vital components of transformer encoders?  
A. Self-attention mechanism and feed-forward artificial neural network  
B. Masked self-attention and encoder-decoder attention  
C. Positional encoding and contextual embeddings  
D. Sequential processing and recursive feedback  

7. **(Hard, Select all that apply)** Which statements about transformer decoders are correct?  
A. They unpack contextualized embeddings back into natural language  
B. They include masked self-attention to limit visibility of future words  
C. They contain encoder-decoder attention mechanisms  
D. They process tokens sequentially like RNNs  

8. **(Medium)** What is the purpose of masked self-attention in the decoder?  
A. To hide certain words from the model completely  
B. To limit the model's ability to see words occurring after a given position in the sequence  
C. To focus only on the first and last words in a sequence  
D. To remove attention completely from certain tokens  

9. **(Hard, Select all that apply)** How do encoders and decoders work together in a transformer architecture?  
A. Encoders produce contextualized embeddings that capture input meaning  
B. Decoders generate output based on the encoder's representations  
C. Both can be stacked or chained together to improve performance  
D. They must always have an equal number of layers  

10. **(Hard)** In the translation example provided in the lecture, what does the encoder-decoder attention mechanism help accomplish?  
A. It ensures the output sentence has the same number of words as the input  
B. It allows the decoder to understand the structure of the original data  
C. It eliminates the need for positional encoding  
D. It processes each word independently  

