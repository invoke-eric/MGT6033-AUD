# Answer Key

## Section 1: BERT and Transfer Learning Fundamentals

1. **Correct Answer:** A  
**Explanation:** BERT stands for "Bidirectional Encoder Representations from Transformers."  
**Verbatim Quote:** "BERT stands for Bidirectional Encoder Representations from Transformers." (7.3b.-Transformers-Applications.txt)

2. **Correct Answers:** A, B, C, D  
**Explanation:** The lecture lists text classification, masked language, semantic similarity, and text generation as key BERT applications.  
**Verbatim Quote:** "Key applications: Text classification, Masked language, Semantic similarity, Text generation." (7.3b.pdf)

3. **Correct Answer:** A  
**Explanation:** Transfer learning with BERT allows fine-tuning pre-trained parameters with a small labeled data sample.  
**Verbatim Quote:** "Transfer learning makes these applications feasible by allowing researchers and analysts to fine-tune pre-trained parameters on a small sample of data." (7.3b.-Transformers-Applications.txt)

## Section 2: BERT Model Applications and Design

4. **Correct Answers:** A, B, D  
**Explanation:** To design a classifier, you add a classification head, fine-tune with labeled data, and use BERTâ€™s output embeddings. Training BERT from scratch is not required.  
**Verbatim Quote:** "To use BERT for classification, we simply add a classification head... We then fine-tune with a relatively small sample of labeled data... We can fine-tune to a dimension that differs from the original purpose..." (7.3b.-Transformers-Applications.txt)

5. **Correct Answer:** A  
**Explanation:** Masked language modeling is used to predict missing words in a sentence.  
**Verbatim Quote:** "Masked language... helps assess the likelihood given words appear in a given context or setting." (7.3b.-Transformers-Applications.txt)

6. **Correct Answers:** A, B, C  
**Explanation:** Masked language modeling can measure entropy, assess text/grammar quality, and identify anomalous documents.  
**Verbatim Quote:** "Key applications: 'Entropy' conveyed in document, 'Quality' of text/grammar, Anomalous documents" (7.3b.pdf)

7. **Correct Answer:** B  
**Explanation:** Masked language models can perpetuate biases present in training data, as shown in the gender/occupation example.  
**Verbatim Quote:** "Mask language models have been used as evidence to show that these complex models appear to perpetuate biases in training data." (7.3b.-Transformers-Applications.txt)

## Section 3: Advanced Applications and Considerations

8. **Correct Answer:** A  
**Explanation:** BERT measures semantic similarity by comparing contextualized embeddings generated by the encoder.  
**Verbatim Quote:** "This will be based on the contextualized embeddings that are generated by the encoder." (7.3b.-Transformers-Applications.txt)

9. **Correct Answers:** A, B, C  
**Explanation:** BERTopic, automatic summarization, and plagiarism detection are all advanced BERT applications; image segmentation is not.  
**Verbatim Quote:** "Automatic summarization... BERTopic... Detecting duplicates... Plagiarism detection" (7.3b.-Transformers-Applications.txt)

10. **Correct Answer:** A  
**Explanation:** Generative models like BERT and GPT are only as good as the text on which they're trained.  
**Verbatim Quote:** "These generative models, like BERT-based ones or even GPT, are only as good as the text on which they're trained." (7.3b.-Transformers-Applications.txt)
