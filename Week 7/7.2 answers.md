# Answer Key

## Section 1: Deep Learning Fundamentals

1. **Correct Answer:** B  
**Explanation:** A network with more than two or three layers is generally considered "deep learning."  
**Verbatim Quote:** "The general rule of thumb is any more than two or three layers leads to this Deep Learning classification." (7.2.-Transfer-Learning.txt)

2. **Correct Answers:** A, B, D  
**Explanation:** Deep learning captures nuanced nonlinearities, each layer adds nonlinearity, and models can have millions of parameters.  
**Verbatim Quote:** "Deep Learning...captures nuanced nonlinearities...every layer in a neural network introduces another order of nonlinearity...many Deep Learning models have millions of parameters..." (7.2.-Transfer-Learning.txt)

3. **Correct Answer:** C  
**Explanation:** Each layer interacts with previous layers, introducing higher-order nonlinearities.  
**Verbatim Quote:** "Adding a second layer interacts all nodes in the first layer, introducing a second degree of nonlinearity. Adding a third and fourth and fifth layers similarly increase the degrees of interactive elements." (7.2.-Transfer-Learning.txt)

## Section 2: Transfer Learning Concepts and Applications

4. **Correct Answer:** B  
**Explanation:** Transfer learning is using a model trained on one task or corpus for a different but related task or corpus.  
**Verbatim Quote:** "Transfer Learning is the process of using a model trained for one reason, with one corpus for one of three other reasons." (7.2.-Transfer-Learning.txt)

5. **Correct Answers:** A, B, C  
**Explanation:** Transfer learning can involve a different reason on the same corpus, a similar reason on a different corpus, or a different reason on a different corpus.  
**Verbatim Quote:** "In NLP, transfer learning is the process of using a model trained for one reason using one corpus for: A different reason in a similar corpus, A similar reason in a different corpus, A different reason in a different corpus" (7.2.pdf)

6. **Correct Answer:** A  
**Explanation:** Transfer learning works because context is similar across most language domains and problems.  
**Verbatim Quote:** "The key idea is that context is similar for most language across most domains and most problems." (7.2.-Transfer-Learning.txt)

7. **Correct Answers:** A, B, C  
**Explanation:** Transfer learning reduces training time and data requirements, and allows starting from a model that already understands language context.  
**Verbatim Quote:** "Greatly reduces training time and data requirements... we're starting somewhere other than a random guess." (7.2.pdf)

8. **Correct Answer:** B  
**Explanation:** Generic sentiment terms like "absolutely fantastic" or "poor" are likely to transfer well.  
**Verbatim Quote:** "Absolutely fantastic. Seems like something that could be used in patient feedback, as does total waste of time or the word poor." (7.2.-Transfer-Learning.txt)

9. **Correct Answer:** B  
**Explanation:** Embeddings and weights for words that do not translate are adjusted during fine-tuning.  
**Verbatim Quote:** "Through the fine tuning process, embeddings and weights related to words that appear to be used similarly will not be updated very much. On the other hand, words that do not translate...may have their weights adjusted." (7.2.-Transfer-Learning.txt)

10. **Correct Answers:** A, B, D  
**Explanation:** Transfer learning is most effective when word context does not change much, few disciplines have unique lexicons, and deep learning models with transfer learning can capture language, context, and syntax for new tasks.  
**Verbatim Quote:** "Transfer Learning works best when the context for most words does not change too drastically. This is usually not a limiting feature as very few disciplines have lexicon that is completely unique...Transfer learning refers to the practice of starting with a trained model for a new task, 'transferring' knowledge about language, context, syntax, and so forth to a new task." (7.2.-Transfer-Learning.txt; 7.2.pdf)
