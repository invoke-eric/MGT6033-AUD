# Quiz Questions

## Section 1: Ensemble Methods Fundamentals

1. **(Easy)** What is an ensemble method in machine learning?  
A. A single model trained on all data  
B. A method that combines predictions from multiple base models to form a consensus  
C. A model that only uses bagging  
D. A method that only uses boosting  

2. **(Medium, Select all that apply):** Which are the three main approaches to ensemble methods discussed in the lecture?  
A. Bagging  
B. Boosting  
C. Stacking  
D. Regularization  

3. **(Medium)** What is the primary advantage of using ensemble methods over a single model?  
A. Ensembles are always faster to train  
B. Ensembles can overcome shortcomings of individual models and improve generalization  
C. Ensembles require less data  
D. Ensembles always guarantee perfect accuracy  

## Section 2: Bagging and Random Forests

4. **(Medium, Select all that apply):** Which statements describe bagging and random forest approaches?  
A. Bagging combines predictions from multiple models using modal classification or average prediction  
B. Random forests train many decision trees on different samples and features  
C. Errors across individual trees in a random forest are mostly uncorrelated  
D. Random forests require all trees to use the same features  

5. **(Hard)** What is a key reason random forests are robust and resistant to overfitting?  
A. Every tree is trained on the same data  
B. Randomization of samples and features makes errors uncorrelated, so averaging reduces overfitting  
C. They have no hyperparameters  
D. They use only a single decision tree  

6. **(Hard, Select all that apply):** What are some advantages of random forests for NLP tasks?  
A. Robustness and accuracy  
B. Resistance to overfitting  
C. Ability to handle missing data  
D. Built-in feature importance estimates  

7. **(Hard)** What is a limitation of random forests mentioned in the lecture?  
A. They cannot handle categorical data  
B. They can be complex and slower to train or predict due to many trees  
C. They always overfit  
D. They do not support parallelization  

## Section 3: Boosting, XGBoost, and LightGBM

8. **(Medium)** How does boosting differ from bagging in ensemble learning?  
A. Boosting trains models sequentially, focusing on hard-to-classify observations  
B. Boosting trains all models in parallel  
C. Boosting uses only random samples  
D. Boosting never uses decision trees  

9. **(Hard, Select all that apply):** Which of the following are true about XGBoost and LightGBM?  
A. Both are refinements of gradient-boosted decision trees  
B. XGBoost grows trees level-wise; LightGBM grows trees leaf-wise  
C. Both incorporate regularization and parallelization  
D. Both are always more interpretable than random forests  

10. **(Hard)** What is a common downside of advanced boosting methods like XGBoost and LightGBM?  
A. They cannot be parallelized  
B. They are less interpretable and require complex tuning of hyperparameters  
C. They always underfit  
D. They cannot handle missing data  

---

