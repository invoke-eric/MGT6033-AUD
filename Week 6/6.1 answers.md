# Answer Key

## Section 1: Supervised Learning Fundamentals

1. **Correct Answer:** B  
**Explanation:** The key distinction is whether the output variable (Y) is continuous (regression) or discrete (classification).  
**Verbatim Quote:** "The key distinction again, is whether the outcome we're trying to predict is a continuous outcome or some discrete category." (6.1-Preparing-To-Train-A-Supervised-Model.txt)

2. **Correct Answers:** A, C  
**Explanation:** Predicting future sales and stock returns are regression-type problems; group membership and failure prediction are classification-type.  
**Verbatim Quote:** "Regression type problems...predicting sales...predicting stock returns..." (6.1.pdf, slide: Supervised Learning Applications in NLP)

3. **Correct Answer:** B  
**Explanation:** Overfitting is when a model predicts training data too well but performs poorly out-of-sample.  
**Verbatim Quote:** "Overfitting...Model predicts training data 'too well'...Model performs poorly out-of-sample" (6.1.pdf, slide: Risks with Supervised Learning)

4. **Correct Answers:** A, B, C  
**Explanation:** Hold-out samples, cross-validation, and reducing complexity are all discussed as strategies to avoid overfitting.  
**Verbatim Quote:** "Strategies to avoid overfitting...Hold-out samples...Cross-validation...Reduce complexity..." (6.1.pdf, slide: Risks with Supervised Learning)

5. **Correct Answer:** B  
**Explanation:** Avoiding contamination of the hold-out sample allows for a true assessment of out-of-sample predictive power.  
**Verbatim Quote:** "It is very important to avoid contaminating any hold-out sample." (6.1.pdf, slide: Summary)

## Section 2: Model Training Workflow and Best Practices

6. **Correct Answers:** A, B, C, D, E  
**Explanation:** The essential steps are: preprocess and prepare data, select model(s), split data, train with cross-validation, and evaluate fit.  
**Verbatim Quote:** "Key Steps...1. Preprocess & prepare the data...2. Select model(s)...3. Split data...4. Train with cross-validation...5. Evaluate model fit" (6.1.pdf, slide: Key Steps)

7. **Correct Answer:** B  
**Explanation:** Random splitting may result in similar observations in both sets, contaminating the hold-out sample.  
**Verbatim Quote:** "Sometimes randomly splitting the data is not appropriate, since certain observations may be very similar to one another along some dimension like a person, or a company, or an industry..." (6.1-Preparing-To-Train-A-Supervised-Model.txt)

8. **Correct Answer:** B  
**Explanation:** Cross-validation minimizes the impact of data-specific nuances on learned parameters.  
**Verbatim Quote:** "Cross-validation minimizes the chances that nuances in the data significantly impact learned parameters." (6.1-Preparing-To-Train-A-Supervised-Model.txt)

9. **Correct Answers:** A, B, C  
**Explanation:** Feature selection, removing irrelevant features, and limiting polynomial degree are all methods to reduce model complexity.  
**Verbatim Quote:** "You can also reduce the complexity of the model...If I restricted the model to be something like a second or a third degree polynomial, it would be unable to learn all of those different ups or downs, which may produce a more generalizable model. Similarly, if I remove irrelevant features, that will also reduce the likelihood of overfitting..." (6.1-Preparing-To-Train-A-Supervised-Model.txt)

10. **Correct Answer:** B  
**Explanation:** Model fit for regression-type problems should include metrics like mean-squared error or R-squared and expert judgment on feature importance.  
**Verbatim Quote:** "Evaluating model fit should always involve evaluating metrics such as the mean-squared error or the R-squared value and regression type problems...we can simply evaluate the coefficients on individual features to see which features most heavily explain the outcomes." (6.1-Preparing-To-Train-A-Supervised-Model.txt)
