# Answer Key

## Section 1: Logit, Probit, and Linear Classifiers

1. **Correct Answer:** A  
**Explanation:** Logit models assume a linear relationship with the log odds; probit models use the cumulative distribution function.  
**Verbatim Quote:** "A logit model assumes a linear relation between the predictors... and the log odds ratio. A Probit assumes a linear relation between the predictors and the cumulative distribution function..." (6.4-Text-based-Classifiers.txt)

2. **Correct Answers:** A, B, C  
**Explanation:** Logit and probit models assume linearity, are susceptible to collinearity, and do not include regularization by default.  
**Verbatim Quote:** "They assume linearity between the input or x variables and some transformation of the outcome. They're also susceptible to collinearity and they have no mechanism for identifying important features or the ability to regularize things." (6.4-Text-based-Classifiers.txt)

3. **Correct Answer:** B  
**Explanation:** They lack mechanisms for feature selection and regularization, which are important in high-dimensional NLP tasks.  
**Verbatim Quote:** "They're also susceptible to collinearity and they have no mechanism for identifying important features or the ability to regularize things." (6.4-Text-based-Classifiers.txt)

## Section 2: Decision Trees

4. **Correct Answer:** A  
**Explanation:** Decision trees emulate a flow-chart style decision process.  
**Verbatim Quote:** "You can view a decision tree as emulating a flow chart type decision making process." (6.4-Text-based-Classifiers.txt)

5. **Correct Answers:** A, B, C  
**Explanation:** Decision trees are easy to implement, capture nonlinearities, and are not sensitive to feature scaling.  
**Verbatim Quote:** "For one, it's easy to implement and it tends to be very fast. It also easily captures nonlinearities... Decision trees are also not subject to issues with scale and distributional assumptions." (6.4-Text-based-Classifiers.txt)

6. **Correct Answers:** A, B, D  
**Explanation:** Decision trees are prone to overfitting, can be biased by unbalanced data, and may require impurity thresholds for leaves.  
**Verbatim Quote:** "First, it's very prone to overfitting. If we don't limit the size of a decision tree, we're always going to end up with a model that perfectly predicts the training data... The model can also be biased by unbalanced data... Often, we want to allow for some, what's called impurity in the leaves of the tree." (6.4-Text-based-Classifiers.txt)

## Section 3: Naive Bayes and Stochastic Gradient Descent

7. **Correct Answer:** A  
**Explanation:** Naive Bayes assumes features are conditionally independent given the class label.  
**Verbatim Quote:** "Assumes features are 'conditionally independent' of class label" (6.4.pdf)

8. **Correct Answers:** A, B, C  
**Explanation:** Naive Bayes is simple, efficient, and works well with high-dimensional data.  
**Verbatim Quote:** "Why useful for text?... simple to implement and interpretability is pretty high... computationally very efficient... works well in various tasks like spam identification." (6.4-Text-based-Classifiers.txt)

9. **Correct Answer:** A  
**Explanation:** The conditional independence assumption is often violated in real-world data.  
**Verbatim Quote:** "Assumes features are 'conditionally independent' of class label... Likely untrue in many cases (spam)" (6.4.pdf)

10. **Correct Answers:** A, B, C  
**Explanation:** SGD is an optimization process, updates parameters using one random observation, is fast, and can incorporate regularization.  
**Verbatim Quote:** "Stochastic gradient descent is technically a process by which parameters are fit for a given model, not a classifier itself. 'Stochastic' = update based on one (random) observation... Very fast... Easily incorporates regularization." (6.4-Text-based-Classifiers.txt)
