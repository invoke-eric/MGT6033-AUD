# Answer Key

## Section 1: Ensemble Methods Fundamentals

1. **Correct Answer:** B  
**Explanation:** Ensemble methods combine predictions from multiple base models to form a consensus prediction or classification.  
**Verbatim Quote:** "An ensemble method refers to using a collection of models in a prediction or classification task... combine multiple base models... to form one consensus estimate or classification." (6.5-Ensemble-Methods-in-NLP.txt)

2. **Correct Answers:** B, C, D  
**Explanation:** Bagging, boosting, and stacking are the three primary ensemble approaches discussed.  
**Verbatim Quote:** "There are three main approaches... Bagging... Boosting... Stacking." (6.5-Ensemble-Methods-in-NLP.txt; 6.5.pdf)

3. **Correct Answer:** C  
**Explanation:** Ensembles can overcome shortcomings of individual models and improve generalization.  
**Verbatim Quote:** "Ensemble methods can overcome shortcomings of individual ML-based classifiers (or predictors)." (6.5.pdf)

## Section 2: Bagging and Random Forests

4. **Correct Answers:** A, B, D  
**Explanation:** Bagging combines predictions from multiple models; random forests train many trees on different samples/features; errors are mostly uncorrelated.  
**Verbatim Quote:** "Bagging... use the modal classification or average prediction as the consensus estimate... random forest... train many individual decision trees... different samples and different features... errors... are mostly uncorrelated." (6.5-Ensemble-Methods-in-NLP.txt)

5. **Correct Answer:** B  
**Explanation:** Randomization of samples/features in random forests makes errors uncorrelated, so averaging reduces overfitting.  
**Verbatim Quote:** "Randomization helps increase the likelihood that prediction errors across individual trees are mostly uncorrelated... the likelihood that we converge on the correct guess increases." (6.5-Ensemble-Methods-in-NLP.txt)

6. **Correct Answers:** A, B, C, D  
**Explanation:** Random forests are robust, accurate, resistant to overfitting, handle missing data, and provide feature importance estimates.  
**Verbatim Quote:** "They're very robust and tend to be pretty accurate... resistant to overfitting... can also handle missing data... built-in feature important estimates..." (6.5-Ensemble-Methods-in-NLP.txt)

7. **Correct Answer:** D  
**Explanation:** Random forests can be complex and slower to train/predict due to many trees.  
**Verbatim Quote:** "Random forests tend to be more complex because they have a large number of trees... can also be a little bit slower to train and to predict new data..." (6.5-Ensemble-Methods-in-NLP.txt)

## Section 3: Boosting, XGBoost, and LightGBM

8. **Correct Answer:** A  
**Explanation:** Boosting trains models sequentially, focusing on observations that are hard to classify.  
**Verbatim Quote:** "In a boosted model, we iteratively re-estimate the model such that observations which are harder to classify are given greater weight." (6.5-Ensemble-Methods-in-NLP.txt)

9. **Correct Answers:** A, B, C  
**Explanation:** Both are refinements of gradient-boosted trees; XGBoost grows trees level-wise, LightGBM leaf-wise; both have regularization and parallelization.  
**Verbatim Quote:** "XGBoost & LightGBM... refinements to Gradient Boosted Trees (GBT)... 'Level' wise (XGBoost)... 'Leaf' wise (LightGBM)... regularization and parallelization." (6.5.pdf; 6.5-Ensemble-Methods-in-NLP.txt)

10. **Correct Answer:** C  
**Explanation:** Advanced boosting methods are less interpretable and require complex tuning of hyperparameters.  
**Verbatim Quote:** "They are not very interpretable, particularly because observation weighting plays a very big role... Tuning these models can also take a while given the complexity of the hyperparameters..." (6.5-Ensemble-Methods-in-NLP.txt)
