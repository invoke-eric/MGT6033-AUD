# Answer Key

## Section 1: Distance and Similarity in Numeric and Text Data

1. **Correct Answer:** B  
**Explanation:** The main objective is to quantify similarity or distance to understand relationships between documents.  
**Verbatim Quote:** "you should be able to quantify similarity across documents in a corpus."  
(*3.5-Comparing-Text.txt*)

2. **Correct Answers:** A, B  
**Explanation:** Both Euclidean and Mahalanobis distances are mentioned as applicable to numeric data.  
**Verbatim Quote:** "mathematical formula we could use, such as the Euclidean distance or Mahalanobis distance."  
(*3.5-Comparing-Text.txt*)

3. **Correct Answer:** C  
**Explanation:** It represents the straight-line (hypotenuse) distance using the Pythagorean theorem.  
**Verbatim Quote:** "Euclidean distance is... the hypotenuse of the triangle connecting dots... Sqrt((10-3)^2 + (6-3)^2) = 7.6"  
(*3.5.pdf*)

4. **Correct Answer:** B  
**Explanation:** Cosine similarity naturally ignores scale (vector magnitude), while Euclidean distance does not.  
**Verbatim Quote:** "Scale is ignored with cosine similarity, but weighs heavily into any distance-based measure."  
(*3.5-Comparing-Text.txt*)

5. **Correct Answers:** A, B, C  
**Explanation:** Cosine similarity values usually range from 0 (no similarity) to 1 (identical). High values mean high similarity and it is computationally efficient.  
**Verbatim Quote:** "the cosine of zero equals one... the closer together the two vectors... the higher the cosine."  
(*3.5-Comparing-Text.txt*)  
**Verbatim Quote:** "cosine similarity can be vectorized pretty easily."  
(*3.5-Comparing-Text.txt*)

## Section 2: Vector Space Models and Document-Term Matrix

6. **Correct Answer:** C  
**Explanation:** Textual data is compared in a k-dimensional vector space, typically using a document-term matrix.  
**Verbatim Quote:** "Concepts are exactly the same, but operating in k-dimensional vector space (can't visualize)."  
(*3.5.pdf*)

7. **Correct Answer:** B  
**Explanation:** Cosine similarity removes the effect of document length, focusing on directional similarity.  
**Verbatim Quote:** "Cosine similarity... naturally addresses differences in length."  
(*3.5.pdf, Summary*)

8. **Correct Answers:** A, B  
**Explanation:** Levenshtein (edit) distance and Jaccard distance are explicitly mentioned as alternatives.  
**Verbatim Quote:** "Levenshtein distance... Jaccard distance."  
(*3.5.pdf, With Textual Data*)

## Section 3: Matrix Algebra and Corpus-Wide Comparison

9. **Correct Answer:** B  
**Explanation:** The diagonal represents each documentâ€™s similarity with itself, which is always 1.  
**Verbatim Quote:** "this is a diagonal matrix... this is not a class in linear algebra... but understanding this format is very useful..."  
(*3.5-Comparing-Text.txt*)

10. **Correct Answers:** A, B, D  
**Explanation:** Cosine similarity is efficient, robust to length, and can be used on large corpora.  
**Verbatim Quote:** "Cosine similarity is particularly useful because it naturally addresses differences in length and is computationally very efficient."  
(*3.5.pdf, Summary*)
