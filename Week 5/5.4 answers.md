# Answer Key

## Section 1: Principles and Motivation for Dimensionality Reduction

1. **Correct Answer:** B  
**Explanation:** Dimensionality reduction aims to reduce the number of variables or columns while retaining important information.  
**Verbatim Quote:** “Dimensionality reduction refers to any strategy for reducing the granularity of how data is described. … we’re reducing the number of columns in our dataset.” (5.4-Strategies-for-reducing-dimensionality.txt)

2. **Correct Answers:** A, B, C, D, F  
**Explanation:** Embeddings, PCA, t-SNE, topic models, and UMAP are all listed as dimensionality reduction methods for NLP; Bag-of-Words is not.  
**Verbatim Quote:** “Options for NLP: Embeddings, Principle Components, Topic models (LDA, NMF, LSA), t-SNE, UMAP” (5.4.pdf)

3. **Correct Answer:** C  
**Explanation:** Multiple techniques are often used together to leverage their strengths for different tasks or stages.  
**Verbatim Quote:** “Suitability of each method depends on task and data. … Often use multiple techniques together.” (5.4.pdf)

## Section 2: Principal Component Analysis (PCA)

4. **Correct Answer:** A  
**Explanation:** PCA relies on singular value decomposition and the eigenvectors of the covariance matrix.  
**Verbatim Quote:** “PCA involves first standardizing the data and then computing a covariance matrix… then we compute what are called the eigenvalues and eigenvectors of the matrix.” (5.4-Strategies-for-reducing-dimensionality.txt)

5. **Correct Answers:** A, B, C, D  
**Explanation:** PCA is simple, fast, deterministic, produces orthogonal components, and can remove noise.  
**Verbatim Quote:** “Pros: Simple & fast, Deterministic, Produces orthogonal components, Can remove noise in data” (5.4.pdf)

6. **Correct Answers:** A, B, C, D  
**Explanation:** PCA’s limitations include interpretability, loss of edge case variation, assumption of linearity, and sensitivity to outliers/scale.  
**Verbatim Quote:** “Cons: Difficult to interpret individual components, May lose important ‘edge case’ variation, Assumes linear relationships, Sensitive to outliers and scale” (5.4.pdf)

## Section 3: Non-linear Dimensionality Reduction and Visualization

7. **Correct Answer:** B  
**Explanation:** t-SNE is primarily used for visualizing high-dimensional data.  
**Verbatim Quote:** “t-Distributed Stochastic Neighbor Embedding … particularly effective for visualizing high-dimensional data.” (5.4.pdf)

8. **Correct Answers:** A, B, C  
**Explanation:** t-SNE is not deterministic, computationally costly, and generally not appropriate for preprocessing.  
**Verbatim Quote:** “Cons: Not deterministic and computationally costly… Generally not appropriate for preprocessing” (5.4.pdf)

9. **Correct Answer:** A  
**Explanation:** UMAP is more scalable and computationally efficient than t-SNE.  
**Verbatim Quote:** “UMAP … is much more flexible, scalable, and computationally efficient.” (5.4-Strategies-for-reducing-dimensionality.txt)

10. **Correct Answer:** D  
**Explanation:** UMAP’s non-linear transformation may distort distances, making clusters less meaningful for distance-based methods.  
**Verbatim Quote:** “It may not work well with other distance-based methods. … you would not want to construct clusters with k-means and then try to use UMAP to collapse those clusters into a lower dimensional space.” (5.4-Strategies-for-reducing-dimensionality.txt)
