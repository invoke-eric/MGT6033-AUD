# Answer Key

## Section 1: K-means Clustering Concepts and Procedure

1. **Correct Answer:** A  
**Explanation:** K-means assumes clusters are spherical because it assigns points to the nearest centroid using Euclidean distance.  
**Verbatim Quote:** "Clusters are spherical" (5.3b.pdf)

2. **Correct Answer:** B  
**Explanation:** Euclidean distance is the standard metric used in K-means to assign points to clusters.  
**Verbatim Quote:** "Euclidean distance must be meaningful" (5.3b.pdf)

3. **Correct Answers:** A, B, C  
**Explanation:** K-means only detects spherical clusters, can be unstable due to random initialization, and can get stuck in local minima; it does not require labeled data.  
**Verbatim Quote:** "Only detects spherical clusters... Can be unstable... Local vs. global minima" (5.3b.pdf)

4. **Correct Answer:** B  
**Explanation:** The first step is to randomly initialize cluster centroids.  
**Verbatim Quote:** "Initiate k centroids" (5.3b.pdf)

5. **Correct Answer:** A  
**Explanation:** The elbow method helps determine the optimal number of clusters by looking for a point where adding more clusters yields diminishing returns.  
**Verbatim Quote:** "Elbow method" (5.3b.pdf)

## Section 2: Evaluating K-means and Clustering Metrics

6. **Correct Answers:** A, B, C  
**Explanation:** WCSS, Calinski-Harabasz, and Davies-Bouldin indices are used to evaluate clustering quality; accuracy is not used for unsupervised clustering.  
**Verbatim Quote:** "Within-cluster sum of squares... Calinski-Harabasz... Davies-Bouldin" (5.3b.pdf)

7. **Correct Answer:** B  
**Explanation:** A silhouette score close to -1 indicates the point is closer to another cluster than its own.  
**Verbatim Quote:** "If we have negative silhouette scores, that implies the point is actually closer to points in another cluster, even though it's been assigned the one that it's in." (5.3b-Clustering-with-K-means-and-HDBSCAN.txt)

8. **Correct Answer:** B  
**Explanation:** With high-dimensional data, K-means may get stuck in a local minimum due to many peaks and valleys in the loss function.  
**Verbatim Quote:** "The loss function for K-means can have lots of peaks and valleys because the data is very high dimensional... we can get stuck in what's called a local valley." (5.3b-Clustering-with-K-means-and-HDBSCAN.txt)

## Section 3: HDBSCAN Clustering Concepts and Comparison

9. **Correct Answer:** A  
**Explanation:** HDBSCAN stands for Hierarchical Density-Based Spatial Clustering of Applications with Noise.  
**Verbatim Quote:** "HDBSCAN stands for Hierarchical Density-based Spatial Clustering of Applications with Noise" (5.3b.pdf)

10. **Correct Answers:** A, B, D  
**Explanation:** HDBSCAN does not require specifying the number of clusters, can handle varying densities, and allows for adjustable granularity; it does not assign all points to clusters.  
**Verbatim Quote:** "Does not require specification of 'k'... can handle different densities... move from more to less granular" (5.3b.pdf)
